- `GroundingDinoSAMSegment (segment anything)`: The GroundingDinoSAMSegment node is designed to leverage the capabilities of the GroundingDINO model for segmenting various objects within an image. It utilizes advanced neural network architectures, including transformers and Swin Transformers, to accurately identify and segment objects by understanding their context and relationships within the image.
    - Inputs:
        - `sam_model` (Required): Specifies the SAM model to be used in conjunction with the GroundingDINO model for the segmentation task. This model assists in refining the segmentation results obtained from GroundingDINO. Type should be `SAM_MODEL`.
        - `grounding_dino_model` (Required): Specifies the GroundingDINO model to be used for initial object detection within the image. This model identifies potential objects of interest based on the provided prompt. Type should be `GROUNDING_DINO_MODEL`.
        - `image` (Required): The input image to be processed. The GroundingDINO model first identifies objects within this image, which are then segmented by the SAM model. Type should be `IMAGE`.
        - `prompt` (Required): A text prompt that guides the GroundingDINO model in identifying objects of interest within the image. This prompt helps focus the model's attention on relevant objects. Type should be `STRING`.
        - `threshold` (Required): A threshold value that determines the sensitivity of object detection by the GroundingDINO model. Objects with confidence scores above this threshold are considered for segmentation. Type should be `FLOAT`.
    - Outputs:
        - `image`: The segmented images obtained after processing. Each segmented object is returned as a separate image. Type should be `IMAGE`.
        - `mask`: The segmentation masks corresponding to each segmented object in the image. These masks indicate the precise area of each object within the original image. Type should be `MASK`.
