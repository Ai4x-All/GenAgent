- `AV_IPAdapter`: The AV_IPAdapter node is designed to integrate and apply IP Adapter models to images within the Art Venture framework. It facilitates the enhancement or alteration of images by leveraging IP Adapter and CLIP vision models, allowing for the customization of visual content based on specified parameters and options.
    - Parameters:
        - `ip_adapter_name`: Specifies the name of the IP Adapter model to be used. This parameter is crucial for selecting the appropriate model for image processing. Type should be `COMBO[STRING]`.
        - `clip_name`: Determines the CLIP vision model to be utilized alongside the IP Adapter model. This parameter is essential for the combined application of both models to the image. Type should be `COMBO[STRING]`.
        - `weight`: Controls the influence of the IP Adapter on the image. This parameter adjusts the strength of the adaptation effect. Type should be `FLOAT`.
        - `noise`: Adjusts the level of noise introduced during the adaptation process. This parameter can be used to fine-tune the visual output. Type should be `FLOAT`.
        - `start_at`: Defines the starting point of the effect applied by the IP Adapter, enabling phased integration over the image. Type should be `FLOAT`.
        - `end_at`: Specifies the end point of the IP Adapter's effect, allowing for a gradual cessation of the adaptation. Type should be `FLOAT`.
        - `weight_type`: Determines the type of weighting applied to the adaptation effect, offering options for standard, prompt importance, or style transfer. Type should be `COMBO[STRING]`.
        - `enabled`: Toggles the application of the IP Adapter and CLIP vision models. When set to false, the adaptation process is bypassed. Type should be `BOOLEAN`.
    - Inputs:
        - `model`: The initial model state before applying the IP Adapter. This parameter represents the starting point for the adaptation process. Type should be `MODEL`.
        - `image`: The image to which the IP Adapter and CLIP vision models will be applied. This parameter is central to the node's functionality, serving as the input for the adaptation process. Type should be `IMAGE`.
        - `ip_adapter_opt`: Optional configurations for the IP Adapter model. This parameter allows for further customization of the adaptation process. Type should be `IPADAPTER`.
        - `clip_vision_opt`: Optional configurations for the CLIP vision model. This parameter enables additional adjustments to how the CLIP model is applied. Type should be `CLIP_VISION`.
        - `attn_mask`: An optional mask that can be applied during the adaptation process, allowing for targeted modifications. Type should be `MASK`.
    - Outputs:
        - `model`: The model after applying the IP Adapter, reflecting the modifications made to the image. Type should be `MODEL`.
        - `pipeline`: A dictionary containing the IP Adapter and CLIP vision models used in the adaptation process. This output provides insight into the models' configurations. Type should be `IPADAPTER`.
        - `clip_vision`: The specific CLIP vision model applied during the adaptation process. This output highlights the role of CLIP vision in the image's transformation. Type should be `CLIP_VISION`.
